{
"title" : "Amazon Web Services Outage Caused By Memory Leak And Failure In Monitoring Alarm",
"date" : "Sat, 27 Oct 2012 08:10:45 +0000",
"author" : "Alex Williams"
}

<img width="100" height="44" src="http://tctechcrunch2011.files.wordpress.com/2011/03/aws-amazon.jpeg?w=100&amp;h=44&amp;crop=1" class="attachment-tc-carousel-river-thumb wp-post-image" alt="aws.amazon" title="aws.amazon" style="float: left; margin: 0 10px 7px 0;" /><p>A memory leak and a failed monitoring system caused the <a href="http://techcrunch.com/2012/10/22/aws-ec2-issues-in-north-virginia-affect-heroku-reddit-and-others-heroku-still-down/">Amazon Web Services outage on Monday</a> that took out Reddit and other major services.</p>
<p>According to a <a target="_blank" href="https://aws.amazon.com/message/680342/">post </a>Friday night, AWS explained that the problem arose after a simple replacement of a data collection server. After installation, the server did not propagate its DNS address correctly and so a fraction of servers did not get the message. Those servers kept trying to reach the server, which led to a memory leak that then went out of control due to the failure of an internal monitoring alarm. Eventually the system ground to a virtual stop and millions of customers felt the pain.</p>
<p>AWS:</p>
<blockquote><p>By Monday morning, the rate of memory loss became quite high and consumed enough memory on the affected storage servers that they were unable to keep up with normal request handling processes.</p></blockquote>
<p>The failure in its North Virginia region eventually interrupted Reddit, Foursquare, Minecraft,  Heroku, GitHub, imgur, Pocket, HipChat, Coursera and a number of others.</p>
<p>In the past, Amazon&#8217;s Elastic Block Storage (EBS) servers have proved troublesome. This outage proved not much different. The EBS servers, feeling the memory leak, began losing the ability to process customer requests, causing the number of stuck volumes to increase quickly. The server degradation came all at once, causing a tax on the system as not enough healthy servers could be found to replace them all.</p>
<p>The outage started at 10 a.m. PST. Five hours later  AWS discovered the root of the problem. An hour later things got back to normal.</p>
<p>AWS says it is taking a number of steps to prevent similar issues going forward. The group plans to  deploy monitoring that will alarm if this specific memory leak problem arises again in any of its production EBS servers. Next week it will begin deploying a fix for the memory leak issue.</p>
<p>AWS has had its share of outages over the past several months. Its problems are magnified by an increasingly competitive market that is seeking to slow AWS momentum by casting doubt on its infrastructure.</p>
<p>I get the competitive issues in play here. But customers should not overlook AWS uniqueness in providing a service that allows startups to use elastic computing, network and storage to compete on the world stage.  It may have outages but no other service comes even close to what AWS offers its customers.</p>
<br /> The original article was posted <a href="http://techcrunch.com/2012/10/27/amazon-web-services-outage-caused-by-memory-leak-and-failure-in-monitoring-alarm/">here</a>.